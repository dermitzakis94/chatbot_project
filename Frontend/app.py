import re
from datetime import datetime
from flask import Flask, request, jsonify, render_template, redirect, url_for
from flask_cors import CORS
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import time

from openai import OpenAI
from dotenv import load_dotenv
from werkzeug.utils import secure_filename
import os

# Î‘Î½ Ï„Î± HTML (index2.html, chatbot.html) ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î¿Î½ Î™Î”Î™ÎŸ Ï†Î¬ÎºÎµÎ»Î¿ Î¼Îµ Ï„Î¿ app.py:
app = Flask(__name__)
CORS(app)

load_dotenv()  # Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ Ï„Î¹Ï‚ Î¼ÎµÏ„Î±Î²Î»Î·Ï„Î­Ï‚ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½Ï„Î¿Ï‚ Î±Ï€ÏŒ Ï„Î¿ .env Î±ÏÏ‡ÎµÎ¯Î¿
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

#ÎŸÏÎ¯Î¶ÎµÎ¹ ÎºÎ±Î¹ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î­Î½Î±Î½ Ï†Î¬ÎºÎµÎ»Î¿ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·Ï‚ Î³Î¹Î± "uploads"
#(Ï€.Ï‡. ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚, Î±ÏÏ‡ÎµÎ¯Î± Ï€Î¿Ï… Î±Î½ÎµÎ²Î¬Î¶Î¿Ï…Î½ Ï‡ÏÎ®ÏƒÏ„ÎµÏ‚ ÏƒÎµ Î¼Î¹Î± web app)
UPLOAD_DIR = os.path.join(os.path.dirname(__file__), "uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

#ÏƒÏ…Î»Î»Î­Î³ÎµÎ¹ ÏŒÎ»Î± Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ Ï„Î¿ website_url

class WebsiteScraper:
    """
    Scraper Ï€Î¿Ï… Ï†Î¿ÏÏ„ÏÎ½ÎµÎ¹ Î¼Î¹Î± ÏƒÎµÎ»Î¯Î´Î± Î¼Îµ Selenium (headless Chrome),
    ÎºÎ¬Î½ÎµÎ¹ Î»Î¯Î³Î¿ scroll Î³Î¹Î± lazy Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿, ÎºÎ±Î¹ ÎµÎ¾Î¬Î³ÎµÎ¹:
    - title
    - meta tags
    - structured_data (JSON-LD)
    - images (absolute - Î³Î¹Î± product context)
    - text (Î¿Î»ÏŒÎºÎ»Î·ÏÎ¿ Î¿ÏÎ±Ï„ÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿)
    - full_html (ÏŒÎ»Î¿ Ï„Î¿ HTML)
    - prices (ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¹Î¼ÏÎ½ â‚¬ Î¼Îµ ÎºÎ¿Î½Ï„Î¹Î½ÏŒ Ï„Î¯Ï„Î»Î¿ - Ï‡Ï‰ÏÎ¯Ï‚ URLs Î³Î¹Î± embedded)
    
    Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ Î³Î¹Î± embedded chatbot - Ï‡Ï‰ÏÎ¯Ï‚ links/scripts/stylesheets
    """

    def __init__(self, url, scroll_passes=3, page_load_timeout=60):
        self.url = url
        self.scroll_passes = scroll_passes
        self.page_load_timeout = page_load_timeout
        self.data = {}
        self.scrape_website()

    # ---------------- internal helpers ----------------
    def setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless=new')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0.0.0 Safari/537.36"
        )
        service = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=service, options=chrome_options)

    @staticmethod
    def _unique(seq):
        # Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· ÏƒÎµÎ¹ÏÎ¬Ï‚ + Î±Ï†Î±Î¯ÏÎµÏƒÎ· Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€Ï‰Î½
        return list(dict.fromkeys(seq or []))

    @staticmethod
    def _extract_meta(soup):
        meta = {}
        for m in soup.find_all("meta"):
            k = m.get("name") or m.get("property") or m.get("http-equiv")
            v = m.get("content")
            if k and v:
                meta[k.strip()] = v.strip()
        return meta

    @staticmethod
    def _extract_structured_data(soup):
        structured = []
        for s in soup.find_all("script", type="application/ld+json"):
            try:
                if s.string:
                    structured.append(json.loads(s.string))
            except Exception:
                structured.append({"raw": s.string})
        return structured

    def _extract_images(self, soup):
        """Î•Î¾Î¬Î³ÎµÎ¹ images Î³Î¹Î± product context (ÏŒÏ‡Î¹ Î³Î¹Î± ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· ÏƒÏ„Î¿ chat)"""
        images = []
        for img in soup.find_all("img"):
            src = img.get("src") or img.get("data-src") or img.get("data-original")
            alt = img.get("alt", "").strip()
            if src:
                img_data = {"src": urljoin(self.url, src)}
                if alt:
                    img_data["alt"] = alt
                images.append(img_data)
        return self._unique([img["src"] for img in images])  # ÎšÏÎ±Ï„Î¬Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î± URLs Î³Î¹Î± ÏƒÏ…Î¼Î²Î±Ï„ÏŒÏ„Î·Ï„Î±

    def extract_prices(self, soup):
        """
        Î•Î½Ï„Î¿Ï€Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¹Î¼ÏÎ½ ÏƒÎµ â‚¬ + ÎºÎ¿Î½Ï„Î¹Î½ÏŒÏ‚ Ï„Î¯Ï„Î»Î¿Ï‚ (best-effort).
        Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹: [{"title","price_raw","price_eur"}] - Î§Î©Î¡Î™Î£ URLs Î³Î¹Î± embedded chatbot
        """
        price_re = re.compile(r'(\d{1,4}(?:[.,]\d{2})?)\s*â‚¬')
        results, seen = [], set()

        for node in soup.find_all(string=price_re):
            m = price_re.search(node)
            if not m:
                continue

            raw = m.group(0)
            num_txt = m.group(1).replace(',', '.')
            try:
                price_val = float(num_txt)
                # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Ï€Î¿Î»Ï Î¼Î¹ÎºÏÏÎ½ Ï„Î¹Î¼ÏÎ½ (Ï€Î¹Î¸Î±Î½ÏÏ‚ ÏŒÏ‡Î¹ Ï€ÏÎ¿ÏŠÏŒÎ½Ï„Î±)
                if price_val < 1:
                    continue
            except Exception:
                continue

            title = None
            card, hops = node.parent, 0
            
            # Î‘Î½Î­Î²Î± Î¼Î­Ï‡ÏÎ¹ 5 Î³Î¿Î½ÎµÎ¯Ï‚ Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Î»Î¿Î³Î¹ÎºÏŒ Ï„Î¯Ï„Î»Î¿
            while card and hops < 5:
                # Î ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î± ÏƒÎµ links Î¼Îµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ (ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ product names)
                a = card.find('a', href=True)
                if a and a.get_text(strip=True) and len(a.get_text(strip=True)) > 3:
                    title = a.get_text(" ", strip=True)[:100]
                    break
                    
                # Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î± headings
                for tag in ['h1','h2','h3','h4','h5','h6']:
                    t = card.find(tag)
                    if t and t.get_text(strip=True):
                        title = t.get_text(" ", strip=True)[:100]
                        break
                if title:
                    break
                
                # Fallback ÏƒÎµ Î¬Î»Î»Î± elements Î¼Îµ Î±ÏÎºÎµÏ„ÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿
                for tag in ['p','span','strong','div']:
                    t = card.find(tag)
                    if t and t.get_text(strip=True) and len(t.get_text(strip=True)) > 5:
                        text = t.get_text(" ", strip=True)[:100]
                        # Î‘Ï€Î¿Ï†Ï…Î³Î® Ï„Î¹Î¼ÏÎ½ Ï‰Ï‚ Ï„Î¯Ï„Î»Î¿Ï…Ï‚
                        if not re.search(r'\d+\s*â‚¬', text):
                            title = text
                            break
                if title:
                    break
                    
                card, hops = card.parent, hops+1

            if not title:
                title = f"Î ÏÎ¿ÏŠÏŒÎ½ {price_val}â‚¬"

            # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¯Ï„Î»Î¿Ï…
            title = re.sub(r'\s+', ' ', title).strip()
            
            key = (title.lower(), price_val)  # Case-insensitive deduplication
            if key not in seen and len(title) > 3:
                seen.add(key)
                results.append({
                    "title": title,
                    "price_raw": raw,
                    "price_eur": price_val
                })

        # Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· ÎºÎ±Ï„Î¬ Ï„Î¹Î¼Î® ÎºÎ±Î¹ Ï€ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î³Î¹Î± embedded context
        results.sort(key=lambda x: x["price_eur"])
        return results[:30]

    # ---------------- main routine ----------------
    def scrape_website(self):
        print(f"ğŸŒ Î¦Î¿ÏÏ„ÏÎ½Ï‰ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ: {self.url}")
        try:
            driver = self.setup_driver()
        except Exception as e:
            print(f"âŒ Chrome driver error: {e}")
            self.data = {"error": str(e), "text": ""}
            return

        try:
            driver.set_page_load_timeout(self.page_load_timeout)
            driver.get(self.url)

            # Î ÎµÏÎ¯Î¼ÎµÎ½Îµ body
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
            except Exception:
                pass

            # Î›Î¯Î³Î¿ scroll Î³Î¹Î± lazy content
            last_h = driver.execute_script("return document.body.scrollHeight")
            for _ in range(self.scroll_passes):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.0)
                new_h = driver.execute_script("return document.body.scrollHeight")
                if new_h == last_h:
                    break
                last_h = new_h

            full_html = driver.page_source
            soup = BeautifulSoup(full_html, "html.parser")

            title = soup.title.string.strip() if soup.title and soup.title.string else None
            meta = self._extract_meta(soup)
            structured_data = self._extract_structured_data(soup)
            images = self._extract_images(soup)
            text = soup.get_text(separator=" ", strip=True)
            prices = self.extract_prices(soup)

            self.data = {
                "source_url": self.url,
                "fetched_at": datetime.utcnow().isoformat() + "Z",
                "title": title,
                "meta": meta,
                "structured_data": structured_data,
                "images": images,  # ÎœÏŒÎ½Î¿ Î³Î¹Î± context, ÏŒÏ‡Î¹ Î³Î¹Î± ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·
                "text": text,
                "full_html": full_html,
                "prices": prices  # Î§Ï‰ÏÎ¯Ï‚ URLs - Î¼ÏŒÎ½Î¿ Ï„Î¯Ï„Î»Î¿Ï‚ ÎºÎ±Î¹ Ï„Î¹Î¼Î®
            }

            print(
                f"âœ… Î£Ï…Î»Î»Î­Ï‡Î¸Î·ÎºÎ±Î½: text({len(text)} chars), "
                f"images({len(images)}), prices({len(prices)} items)"
            )

        except Exception as e:
            print(f"âŒ Scraping error: {e}")
            self.data = {"error": str(e), "text": ""}
        finally:
            try:
                driver.quit()
            except Exception:
                pass


CONV_HISTORY = {}

##ÏŒÎ»Î± Ï„Î± Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î± routes
@app.route('/', methods=['GET'])
def root():
    # Î¡Î¯Î¶Î± -> Ï†ÏŒÏÎ¼Î±
    return redirect(url_for('form_page'))

@app.route('/form', methods=['GET'])
def form_page():
    # Î ÏÎ¿Î²Î¬Î»ÎµÎ¹ Ï„Î· Ï†ÏŒÏÎ¼Î±
    return render_template('index.html')

@app.route('/chatbot/<company_id>', methods=['GET'])
def chatbot_page(company_id):
    # Î ÏÎ¿Î²Î¬Î»ÎµÎ¹ Ï„Î¿ chatbot (Ï„Î¿ company_id Ï„Î¿ ÎºÏÎ±Ï„Î¬Î¼Îµ Î³Î¹Î± Ï„Î· ÏÎ¿Î® ÏƒÎ¿Ï…)
    return render_template('chatbot.html')

@app.route('/health', methods=['GET'])
def health():
    return jsonify(status='ok'), 200

#Ï€Î±Î¯ÏÎ½ÎµÎ¹ Ï„Î¹Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ Ï„Î·Ï‚ Ï†ÏŒÏÎ¼Î±Ï‚ Ï„Î¹Ï‚ Î¼ÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ ÏƒÎµ json ÎºÎ±Î¹ Ï„Î¹Ï‚ Î²Î¬Î¶ÎµÎ¹ ÏƒÎµ dict

@app.route("/create-assistant", methods=["POST"])
def create_assistant():
    f = request.form
    files = request.files.getlist("files")  # Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ []

    # Î‘Î½ Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Î´Î¹Î¬Î»ÎµÎ¾Îµ "Î†Î»Î»Î¿/Other", Ï€Î­ÏÎ½Î± Ï„Î·Î½ Ï„Î¹Î¼Î® Î±Ï€ÏŒ Ï„Î¿ extra Ï€ÎµÎ´Î¯Î¿
    industry = (f.get("industry") or f.get("industryOther") or "").strip()
    required_keys = [
        "companyName", "websiteURL", "description", "questions",
        "endMessage", "greeting", "keywords"
    ]
    missing = [k for k in required_keys if not (f.get(k) or "").strip()]
    if not industry:
        missing.append("industry")

    if missing:
        return jsonify({"status": "error", "missing": missing}), 400

    # Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ® Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Ï‰Î½ (Î±Î½ ÏƒÏ„Î¬Î»Î¸Î·ÎºÎ±Î½)
    saved_files = []
    for file in files:
        if file and file.filename:
            filename = secure_filename(file.filename)
            path = os.path.join(UPLOAD_DIR, filename)
            file.save(path)
            saved_files.append(filename)

    payload = {
        "companyName":     f.get("companyName"),
        "industry":        industry,
        "websiteURL":      f.get("websiteURL"),
        "description":     f.get("description"),
        "questions":       f.get("questions"),
        "endMessage":      f.get("endMessage"),
        "greeting":        f.get("greeting"),
        "keywords":        f.get("keywords"),
        "files_saved":     saved_files,      # [] Î±Î½ Î´ÎµÎ½ Î±Î½Î­Î²Î·ÎºÎµ Ï„Î¯Ï€Î¿Ï„Î±
        "files_count":     len(saved_files)
    }

    scraped = None
    if payload["websiteURL"]:
        try:
            scraper = WebsiteScraper(payload["websiteURL"])
            scraped = scraper.data
            print(f"ğŸ§© Scraped: text({len(scraped.get('text',''))}), images({len(scraped.get('images',[]))}), prices({len(scraped.get('prices',[]))})")
        except Exception as e:
            print("âŒ Scrape error:", e)
            scraped = {"error": str(e)}

    print("ğŸ”¥ /create-assistant received:", payload)
    return jsonify({"status": "ok", "received": payload,"scraped":scraped}), 200
@app.route("/chat", methods=["POST"])
def chat():
    try:
        data = request.get_json(force=True)
    except Exception as e:
        return jsonify({"error": "bad_json", "detail": str(e)}), 400

    session_id = (data.get("session_id") or "default").strip()
    question   = (data.get("question") or "").strip()
    website_data = data.get("website_data") or {}
    company_info = data.get("company_info") or {}
    evaluation_context = data.get("evaluation_context") or {}

    if not question:
        return jsonify({"error": "empty_question"}), 400

    # init history
    if session_id not in CONV_HISTORY:
        CONV_HISTORY[session_id] = []

    # ---------- Normalize fields ----------
    ci = company_info or {}
    company_name = (ci.get('companyName') or ci.get('name') or '').strip()
    industry     = (ci.get('industry') or '').strip()
    greeting     = (ci.get('greeting') or 'Î“ÎµÎ¹Î± ÏƒÎ±Ï‚! Î ÏÏ‚ Î¼Ï€Î¿ÏÏ Î½Î± Î²Î¿Î·Î¸Î®ÏƒÏ‰;').strip()
    farewell     = (ci.get('endMessage') or '').strip()

    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ ÎµÎ¯Î½Î±Î¹ Î· Ï€ÏÏÏ„Î· ÎµÏÏÏ„Î·ÏƒÎ· Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î·
    # Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ history ÎšÎ‘Î˜ÎŸÎ›ÎŸÎ¥, ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Ï„Î¿ frontend Î­Ï‡ÎµÎ¹ Î®Î´Î· ÏƒÏ„ÎµÎ¯Î»ÎµÎ¹ Ï‡Î±Î¹ÏÎµÏ„Î¹ÏƒÎ¼ÏŒ
    # Î¿Ï€ÏŒÏ„Îµ Î´ÎµÎ½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î½Î± Ï‡Î±Î¹ÏÎµÏ„Î®ÏƒÎ¿Ï…Î¼Îµ Î¾Î±Î½Î¬
    has_any_history = len(CONV_HISTORY.get(session_id, [])) > 0
    is_first_user_question = not has_any_history

    # ---------- Build context from website_data fields (Î§Î©Î¡Î™Î£ links/scripts/stylesheets) ----------
    INCLUDE_FULL_HTML = bool(evaluation_context.get("include_full_html", False))

    context = {
        "source_url":      website_data.get("source_url") or website_data.get("url"),
        "title":           website_data.get("title"),
        "meta":            website_data.get("meta"),
        "structured_data": website_data.get("structured_data"),
        "images":          website_data.get("images"),    # ÎœÏŒÎ½Î¿ Î³Î¹Î± context
        "text":            website_data.get("text"),      # ÎŸÎ»ÏŒÎºÎ»Î·ÏÎ¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿
        "prices":          website_data.get("prices"),    # Î§Ï‰ÏÎ¯Ï‚ URLs
    }
    
    if INCLUDE_FULL_HTML and website_data.get("full_html"):
        # Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒÏ‚ "ÎºÏŒÏ†Ï„Î·Ï‚" Î³Î¹Î± Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±
        MAX_HTML_CHARS = int(evaluation_context.get("max_full_html_chars", 20000))
        context["full_html"] = website_data["full_html"][:MAX_HTML_CHARS]

    # jsonify ALL context for Ï„Î¿ prompt
    context_json = json.dumps(context, ensure_ascii=False)

    # ---------- SYSTEM PROMPT - Î”Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ Î³Î¹Î± Ï€ÏÏÏ„Î· vs ÎµÏ€ÏŒÎ¼ÎµÎ½ÎµÏ‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ----------
    if is_first_user_question:
        # Î ÏÏÏ„Î· ÎµÏÏÏ„Î·ÏƒÎ·: Î”Î•Î Ï‡Î±Î¹ÏÎµÏ„Î¬Î¼Îµ Î³Î¹Î±Ï„Î¯ Ï„Î¿ frontend Î­Ï‡ÎµÎ¹ Î®Î´Î· Ï‡Î±Î¹ÏÎµÏ„Î¯ÏƒÎµÎ¹
        system_prompt = f"""
You are the customer-support AI agent for the company "{company_name or 'â€”'}"{(' in the ' + industry + ' industry') if industry else ''}.

â€¢ Always answer in Greek, concise and practical.
â€¢ Do NOT include any greeting - a greeting has already been sent by the system.
â€¢ Simply answer the user's question directly.
â€¢ If the user indicates they are done or says thanks, close with: "{farewell or 'Î£Î±Ï‚ ÎµÏ…Ï‡Î±ÏÎ¹ÏƒÏ„Î¿ÏÎ¼Îµ Î³Î¹Î± Ï„Î¿Î½ Ï‡ÏÏŒÎ½Î¿ ÏƒÎ±Ï‚.'}".

Use ONLY the website data context below for factual information. Do not invent facts.

For general inquiries:
â€¢ Use the website text and structured data to provide accurate information
â€¢ Be helpful but stay within the bounds of available information
â€¢ If information isn't available, politely say so and offer alternatives

Website data context:
{context_json}
""".strip()
    else:
        # Î•Ï€ÏŒÎ¼ÎµÎ½ÎµÏ‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚: Î§Ï‰ÏÎ¯Ï‚ Ï‡Î±Î¹ÏÎµÏ„Î¹ÏƒÎ¼ÏŒ
        system_prompt = f"""
You are the customer-support AI agent for the company "{company_name or 'â€”'}"{(' in the ' + industry + ' industry') if industry else ''}.

â€¢ Always answer in Greek, concise and practical.
â€¢ Do NOT include any greeting - continue the existing conversation.
â€¢ If the user indicates they are done or says thanks, close with: "{farewell or 'Î£Î±Ï‚ ÎµÏ…Ï‡Î±ÏÎ¹ÏƒÏ„Î¿ÏÎ¼Îµ Î³Î¹Î± Ï„Î¿Î½ Ï‡ÏÏŒÎ½Î¿ ÏƒÎ±Ï‚.'}".

Use ONLY the website data context below for factual information. Do not invent facts.

For general inquiries:
â€¢ Use the website text and structured data to provide accurate information
â€¢ Be helpful but stay within the bounds of available information
â€¢ If information isn't available, politely say so and offer alternatives

Website data context:
{context_json}
""".strip()

    # ---------- Build messages ----------
    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(CONV_HISTORY[session_id])
    messages.append({"role": "user", "content": question})

    # ---------- Call OpenAI with safe error handling ----------
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.7,
        )
        answer = resp.choices[0].message.content or ""
    except Exception as e:
        # Î ÏÎ¿ÏƒÏ€Î¬Î¸Î·ÏƒÎµ fallback Î±Î½ Î­ÏƒÏ„ÎµÎ¹Î»ÎµÏ‚ full_html ÎºÎ±Î¹ "Î¾Î­Ï†Ï…Î³Îµ"
        if INCLUDE_FULL_HTML:
            try:
                context.pop("full_html", None)
                context_json = json.dumps(context, ensure_ascii=False)
                fallback_prompt = system_prompt.split("Website data context")[0] + f"Website data context (NO full_html):\n{context_json}"
                messages = [{"role": "system", "content": fallback_prompt}] + CONV_HISTORY[session_id] + [{"role": "user", "content": question}]
                resp = client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    temperature=0.7,
                )
                answer = resp.choices[0].message.content or ""
            except Exception as e2:
                print(f"[CHAT ERROR fallback] {e2}")
                return jsonify({"error": "chat_failed", "detail": str(e2)}), 500
        else:
            print(f"[CHAT ERROR] {e}")
            return jsonify({"error": "chat_failed", "detail": str(e)}), 500

    # ---------- Save history ----------
    CONV_HISTORY[session_id].append({"role": "user", "content": question})
    CONV_HISTORY[session_id].append({"role": "assistant", "content": answer})

    return jsonify({"answer": answer, "history": CONV_HISTORY[session_id]}), 200

    # ---------- Save history ----------
    CONV_HISTORY[session_id].append({"role": "user", "content": question})
    CONV_HISTORY[session_id].append({"role": "assistant", "content": answer})

    return jsonify({"answer": answer, "history": CONV_HISTORY[session_id]}), 200
if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(debug=True, port=port)
